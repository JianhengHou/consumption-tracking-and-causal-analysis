{"cells":[{"cell_type":"code","source":["# '''\n# Sync Command:\n# aws s3 sync s3://datastream-prd-client-alphaedison s3://ae-disqo-data-storage-oregan/raw_data/Disqo_Dataset_Synced --source-region us-east-1 --region us-west-2\n# '''"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c710b733-976e-46c1-9b70-a460fdb9e162","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[1]: &#39;\\nSync Command:\\naws s3 sync s3://datastream-prd-client-alphaedison s3://ae-disqo-datalake/oregon-prod/472754338013297/mnt/delta/Disqo_Dataset_Synced --source-region us-east-1 --region us-west-1\\n&#39;</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[1]: &#39;\\nSync Command:\\naws s3 sync s3://datastream-prd-client-alphaedison s3://ae-disqo-datalake/oregon-prod/472754338013297/mnt/delta/Disqo_Dataset_Synced --source-region us-east-1 --region us-west-1\\n&#39;</div>"]}}],"execution_count":0},{"cell_type":"code","source":["from collections import defaultdict\nimport re\nfrom textdistance import smith_waterman, jaccard\nfrom math import ceil\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\nfrom pyspark.sql.functions import to_timestamp, current_date, lit, year, month, col, count, to_date, floor as floor_, datediff, row_number, when, avg\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.types import DoubleType, LongType\nimport zipcodes\nfrom datetime import date, timedelta, datetime\nfrom pytz import timezone\nimport pytz"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b0926118-b01b-4ca5-ba1a-25766bc49299","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# dbutils.fs.ls(\"/mnt/delta/Disqo_Dataset_Synced/active-users/\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"a151a703-78de-4b57-8c0b-48f7347b085a","inputWidgets":{},"title":"Check Dataset Update for Active Users Demographics "}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# dbutils.fs.ls(\"/mnt/delta/general_data/\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"a9b7405c-176f-4ad9-9cc3-df14df51bc86","inputWidgets":{},"title":"Check Dataset Update for Event Logging"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[4]: [FileInfo(path=&#39;dbfs:/mnt/delta/general_data/CDI/&#39;, name=&#39;CDI/&#39;, size=0, modificationTime=1655229307126),\n FileInfo(path=&#39;dbfs:/mnt/delta/general_data/Disqo_Dataset_Processed_Delta/&#39;, name=&#39;Disqo_Dataset_Processed_Delta/&#39;, size=0, modificationTime=1655229307126),\n FileInfo(path=&#39;dbfs:/mnt/delta/general_data/Disqo_Dataset_Synced/&#39;, name=&#39;Disqo_Dataset_Synced/&#39;, size=0, modificationTime=1655229307126),\n FileInfo(path=&#39;dbfs:/mnt/delta/general_data/disco_data_politics/&#39;, name=&#39;disco_data_politics/&#39;, size=0, modificationTime=1655229307126),\n FileInfo(path=&#39;dbfs:/mnt/delta/general_data/disqo_CDI_models/&#39;, name=&#39;disqo_CDI_models/&#39;, size=0, modificationTime=1655229307126),\n FileInfo(path=&#39;dbfs:/mnt/delta/general_data/disqo_foodfocus_1/&#39;, name=&#39;disqo_foodfocus_1/&#39;, size=0, modificationTime=1655229307126),\n FileInfo(path=&#39;dbfs:/mnt/delta/general_data/disqo_survey1/&#39;, name=&#39;disqo_survey1/&#39;, size=0, modificationTime=1655229307126),\n FileInfo(path=&#39;dbfs:/mnt/delta/general_data/disqo_survey1_youtube/&#39;, name=&#39;disqo_survey1_youtube/&#39;, size=0, modificationTime=1655229307126),\n FileInfo(path=&#39;dbfs:/mnt/delta/general_data/hot_words_delta/&#39;, name=&#39;hot_words_delta/&#39;, size=0, modificationTime=1655229307126),\n FileInfo(path=&#39;dbfs:/mnt/delta/general_data/plots/&#39;, name=&#39;plots/&#39;, size=0, modificationTime=1655229307126)]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[4]: [FileInfo(path=&#39;dbfs:/mnt/delta/general_data/CDI/&#39;, name=&#39;CDI/&#39;, size=0, modificationTime=1655229307126),\n FileInfo(path=&#39;dbfs:/mnt/delta/general_data/Disqo_Dataset_Processed_Delta/&#39;, name=&#39;Disqo_Dataset_Processed_Delta/&#39;, size=0, modificationTime=1655229307126),\n FileInfo(path=&#39;dbfs:/mnt/delta/general_data/Disqo_Dataset_Synced/&#39;, name=&#39;Disqo_Dataset_Synced/&#39;, size=0, modificationTime=1655229307126),\n FileInfo(path=&#39;dbfs:/mnt/delta/general_data/disco_data_politics/&#39;, name=&#39;disco_data_politics/&#39;, size=0, modificationTime=1655229307126),\n FileInfo(path=&#39;dbfs:/mnt/delta/general_data/disqo_CDI_models/&#39;, name=&#39;disqo_CDI_models/&#39;, size=0, modificationTime=1655229307126),\n FileInfo(path=&#39;dbfs:/mnt/delta/general_data/disqo_foodfocus_1/&#39;, name=&#39;disqo_foodfocus_1/&#39;, size=0, modificationTime=1655229307126),\n FileInfo(path=&#39;dbfs:/mnt/delta/general_data/disqo_survey1/&#39;, name=&#39;disqo_survey1/&#39;, size=0, modificationTime=1655229307126),\n FileInfo(path=&#39;dbfs:/mnt/delta/general_data/disqo_survey1_youtube/&#39;, name=&#39;disqo_survey1_youtube/&#39;, size=0, modificationTime=1655229307126),\n FileInfo(path=&#39;dbfs:/mnt/delta/general_data/hot_words_delta/&#39;, name=&#39;hot_words_delta/&#39;, size=0, modificationTime=1655229307126),\n FileInfo(path=&#39;dbfs:/mnt/delta/general_data/plots/&#39;, name=&#39;plots/&#39;, size=0, modificationTime=1655229307126)]</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["# Demo Data Processing"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4a0d556d-f879-4dde-8c0f-ec5681a79913","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["def convertState(x):\n  if x is None: return x\n  x = x.strip()\n  return zipcodes.matching(x)[0]['state'] if len(x) == 5 and len(zipcodes.matching(x))>0 else None\ndef convertCity(x):\n  if x is None: return x\n  x = x.strip()\n  return zipcodes.matching(x)[0]['city'] if len(x) == 5 and len(zipcodes.matching(x))>0 else None\n\ndef convertUrbanization(x, urban_rural_mapping):\n  if x is None: return 'rural'\n  try:\n    x = int(x.strip())\n  except:\n    return None\n  return urban_rural_mapping[x] if x in urban_rural_mapping else 'rural'\n\ndef household_income_process(x):\n  if x is None: return \"#N_A\"\n  if '$' in x: return x\n  else: return '#N_A'\n  \ndef region_process(x):\n  if x in set(['WA', 'OR', 'AK', 'HI', 'MT', 'WY', 'ID', 'CA', 'NV', 'UT', 'CO']):\n    return 'WEST'\n  elif x in set(['AZ', 'NM', 'TX', 'OK']):\n    return 'SOUTHWEST'\n  elif x in set(['ND', 'SD', 'NE', 'KS', 'MN', 'IA', 'MO', 'WI', 'IL', 'IN', 'MI', 'OH']):\n    return 'MIDWEST'\n  elif x in set(['AR', 'LA', 'MS', 'AL', 'TN', 'KY', 'WV', 'DC', 'VA', 'NC', 'SC', 'GA', 'FL', 'DE', 'MD']):\n    return 'SOUTHEAST'\n  elif x in set(['PA', 'NY', 'NJ', 'CT', 'VT', 'ME', 'NH', 'MA', 'RI']):\n    return 'NORTHEAST'\n  \nimport pandas as pd\ndf = pd.read_csv(\"/dbfs/FileStore/tables/disqo_dataset/rural_urban.txt\")\nurban_rural_mapping = {}\nfor row in df.iterrows():\n  urban_rural_mapping[row[1]['ZCTA5']] = row[1]['urban_rural']\n\nstateUDF = udf(lambda z: convertState(z),StringType())\ncityUDF = udf(lambda z: convertCity(z),StringType())\nurbanizationUDF = udf(lambda z: convertUrbanization(z, urban_rural_mapping), StringType())\nhousehold_incomeUDF = udf(lambda z: household_income_process(z), StringType())\nregionUDF = udf(lambda z: region_process(z), StringType())\n\n# # load and read raw demo data\ndemo_data_path = \"/mnt/delta/raw_data/Disqo_Dataset_Synced/v2/active-users/*/*.csv.gz\"\ndemo_data = spark.read.option(\"header\", \"true\").csv(demo_data_path).dropDuplicates()\n\n# # Manipulate date of birth and age\ndemo_data = demo_data.withColumn(\"age\", year(current_date()) - year(demo_data.birth_year))\ndemo_data = demo_data.withColumn(\"state\", stateUDF(col(\"zip_postal_code\")))\ndemo_data = demo_data.withColumn(\"city\", cityUDF(col(\"zip_postal_code\")))\ndemo_data = demo_data.withColumn(\"urbanization\", urbanizationUDF(col(\"zip_postal_code\")))\ndemo_data = demo_data.withColumn(\"region\", regionUDF(col(\"state\")))\n\ndemo_data = demo_data.withColumn(\"household_income\", household_incomeUDF(col('household_income')))\ndemo_data = demo_data.select(col('*'), row_number().over(Window.partitionBy(\"user_id\").orderBy(col(\"household_income\").desc())).alias('rowNumber'))\ndemo_data = demo_data.filter(col('rowNumber')==1).drop('rowNumber')\n\n# save this demo data into delta format\nmain_demo_data_delta_file_path = \"/mnt/delta/general_data/Disqo_Dataset_Processed_Delta/disqo_demo_data\"\ndemo_data.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(main_demo_data_delta_file_path)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"654855cb-40da-4229-8b55-ec12d414f403","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["%sql\ndrop table disqo_demo_data;\n-- Headsup: Delete the exsisting table before creating this one\ncreate TABLE disqo_demo_data \nUSING delta\nLOCATION \"/mnt/delta/general_data/Disqo_Dataset_Processed_Delta/disqo_demo_data\""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"implicitDf":true},"nuid":"3316c982-d38f-4ab9-9244-6d775764b78b","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def preprocess_income(income):\n  if income == \"Under $5,000\":\n    return '0-40k'\n  elif income == \"Under $10,000\":\n    return '0-40k'\n  elif income == \"$10,000 - $14,999\":\n    return '0-40k'\n  elif income == \"$15,000 - $19,999\":\n    return '0-40k'\n  elif income == \"$10,000 - $14,999\":\n    return '0-40k'\n  elif income == \"$20,000 - $24,999\":\n    return '0-40k'\n  elif income == \"$25,000 - $29,999\":\n    return '0-40k'\n  elif income == \"$30,000 - $34,999\":\n    return '0-40k'\n  elif income == \"$35,000 - $39,999\":\n    return '0-40k'\n  elif income == \"$40,000 - $44,999\":\n    return '40k-100k'\n  elif income == \"$45,000 - $49,999\":\n    return '40k-100k'\n  elif income == \"$50,000 - $54,999\":\n    return '40k-100k'\n  elif income == \"$55,000 - $59,999\":\n    return '40k-100k'\n  elif income == \"$60,000 - $64,999\":\n    return '40k-100k'\n  elif income == \"$65,000 - $69,999\":\n    return '40k-100k'\n  elif income == \"$65,000 - $69,999\":\n    return '40k-100k'\n  elif income == \"$70,000 - $74,999\":\n    return '40k-100k'\n  elif income == \"$75,000 - $79,999\":\n    return '40k-100k'\n  elif income == \"$80,000 - $84,999\":\n    return '40k-100k'\n  elif income == \"$85,000 - $89,999\":\n    return '40k-100k'\n  elif income == \"$90,000 - $94,999\":\n    return '40k-100k'\n  elif income == \"$100,000 - $124,999\":\n    return '100k-200k'\n  elif income == \"$125,000 - $149,999\":\n    return '100k-200k'\n  elif income == \"$150,000 - $174,999\":\n    return '100k-200k'\n  elif income == \"$175,000 - $199,999\":\n    return '100k-200k'\n  elif income == \"$200,000 - $249,999\":\n    return '200k+'\n  elif income == \"More than $250,000\":\n    return '200k+'\n  else:\n    return '40k-100k'\n\ndef process_presence_of_children(children):\n  if children == 'ZERO':\n    return 'no_children'\n  elif children == 'ONE':\n    return \"1_child\"\n  elif children == 'TWO':\n    return '2-3_children'\n  elif children == 'THREE':\n    return '2-3_children'\n  elif children == 'FOUR':\n    return '>3_children'\n  elif children == 'FIVE':\n    return '>3_children'\n  elif children == 'SIX':\n    return '>3_children'\n  elif children == 'SEVEN':\n    return '>3_children'\n  elif children == 'EIGHT':\n    return '>3_children'\n  else: \n    return 'no_children'\n\ndef process_education_level(level):\n  if level == 'THIRD_GRADE_OR_LESS' or level == 'MIDDLE_SCHOOL_GRADES_FOUR_TO_EIGHT' or level ==  'COMPLETED_SOME_HIGH_SCHOOL':\n    return 'BelowHighSchool' # Less than a high school degree\n  elif level == 'HIGH_SCHOOL_GRADUATE' or level == 'VOCATIONAL_TRAINING_OR_TRADE_SCHOOL' or level == 'SOME_COLLEGE_OR_UNIVERSITY':\n    return 'HighSchool' # High school degree\n  elif level == 'ASSOCIATE_TWO_YEAR_DEGREE' or level == 'BACHELOR_DEGREE' or level == 'SOME_POSTGRADUATE_STUDY':\n    return 'AssociateOrBachelor' # Associate Degree or Bachelor’s degree\n  elif level == 'MASTER_DEGREE' or level == 'DOCTORATE_OR_PHD':\n    return 'MasterOrDoctor' # Master’s degree oor Doctorate's degree\n  else:\n    return 'UnkownEducationLevel'\n  \ndef process_employment_status(level):\n  if level == 'STUDENT_PART_TIME' or level == 'STUDENT_FULL_TIME':\n    return 'STUDENT'\n  elif level == 'ACTIVE_MILITARY' or level ==  'INACTIVE_MILITARY_OR_VETERAN' or level == 'HOMEMAKER_OR_STAY_AT_HOME_PARENT' or level == 'RETIRED' or 'PERMANENTLY_UNEMPLOYED_OR_DISABLED':\n    return 'Out_of_WORK_FORCE'\n  elif level == 'SELF_EMPLOYED_PART_TIME' or level == 'SELF_EMPLOYED_FULL_TIME' or level == 'EMPLOYED_PART_TIME' or level == 'EMPLOYED_FULL_TIME':\n    return 'EMPLOYED'\n  elif level == 'UNEMPLOYED':\n    return 'UNEMPLOYED'\n  else:\n    return 'UNKNOWN_OR_OTHER'\n\ndef process_marital_status(level):\n  if level == 'SEPARATED' or level == 'SINGLE' or level == 'DIVORCED' or level == 'WIDOWED':\n    return 'LiveAlone'  # Live Alone\n  elif level == 'DOMESTIC_PARTNERSHIP' or level ==  'MARRIED':\n    return 'LiveWithPartners'  # Live with Partners\n  else:\n    return 'UnkownMartitalStatus'\n\ndef process_living_status(level):\n  if level == 'RENT_APARTMENT_OR_CONDO' or level == 'RENT_HOME' or level == 'UNIVERSITY_RESIDENCE' or level == 'RENT':\n    return 'RENTER'\n  elif level == 'OWN_HOME' or level ==  'OWN_APARTMENT_OR_CONDO':\n    return 'OWNER'\n  elif level == 'LIVE_WITH_PARENTS_OR_RELATIVES':\n    return 'WITH_FAMILY'\n  else:\n    return 'UNKOWN_OR_FARM'\n\ndef process_ethnicity(level):\n  if level == 'ASIAN_VIETNAMESE' or level == 'ASIAN_KOREAN' or level == 'ASIAN_JAPANESE' or level == 'ASIAN_INDIAN' or level == 'ASIAN_FILIPINO' or level == 'ASIAN_CHINESE' or level == 'ASIAN_OTHER' or level ==  'ASIAN':\n    return 'ASIAN'\n  elif level == 'WHITE_OR_CAUCASIAN':\n    return 'WHITE_OR_CAUCASIAN'\n  elif level == 'BLACK_OR_AFRICAN_AMERICAN':\n    return 'BLACK_OR_AFRICAN_AMERICAN'\n  elif level == 'PACIFIC_ISLANDER_SAMOAN' or level == 'PACIFIC_ISLANDER_NATIVE_HAWAIIAN' or level == 'PACIFIC_ISLANDER_GUAMANIAN' or level == 'PACIFIC_ISLANDER_OTHER' or level == 'MIDDLE_EASTERN' or level == 'AMERICAN_INDIAN_OR_ALASKAN_NATIVE' or level == 'MIXED_OR_OTHER_RACE':\n    return 'MIXED_OR_OTHER_RACE'\n  else:\n    return 'UNKNOWN'\n  \ndef process_age(level):\n  if level <= 27:\n    return '<28'\n  elif  level <= 40:\n    return '28-40'\n  elif level <= 58:\n    return '41-58'\n  elif level > 58:\n    return '>58'\n  return '28-40' \n\ndef process_age_group_for_weighting(level):\n    if level <= 10:\n        return '0-10'\n    elif level <= 17:\n        return '11-17'\n    elif level <= 20:\n        return '18-20'\n    elif level <= 30:\n        return '21-30'\n    elif level <= 40:\n        return '31-40'\n    elif level <= 50:\n        return '41-50'\n    elif level <= 60:\n        return '51-60'\n    elif level <= 70:\n        return '61-70'\n    elif level <= 80:\n        return '71-80'\n    elif level <= 200:\n        return '81-90'\n    return random.choice(['18-20', '21-30', '31-40', '41-50'])"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0edc347e-da67-4788-ac13-dd6940726977","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# UDF functions\npreprocess_income_UDF = udf(lambda z: preprocess_income(z), StringType())  \nprocess_education_level_UDF = udf(lambda z: process_education_level(z), StringType())  \nprocess_employment_status_UDF = udf(lambda z: process_employment_status(z), StringType())  \nprocess_living_status_UDF = udf(lambda z: process_living_status(z), StringType())  \nprocess_ethnicity_UDF = udf(lambda z: process_ethnicity(z), StringType())  \nprocess_presence_of_children_UDF = udf(lambda z: process_presence_of_children(z), StringType())  \nprocess_age_UDF = udf(lambda z: process_age(z), StringType()) \nprocess_age_group_for_weighting_UDF = udf(lambda x: process_age_group_for_weighting(x), StringType())"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8d9d9a84-686f-4b81-a8f3-e61f285eaa06","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["processed_demo_data_df = demo_data.withColumn('household_income', preprocess_income_UDF(col('household_income')))\nprocessed_demo_data_df = processed_demo_data_df.na.fill({'age': processed_demo_data_df.select(avg('age')).collect()[0][\"avg(age)\"]})\nprocessed_demo_data_df = processed_demo_data_df.withColumn('age_group', process_age_group_for_weighting_UDF(col('age')))\nprocessed_demo_data_df = processed_demo_data_df.withColumn('age', process_age_UDF(col('age')))\nprocessed_demo_data_df = processed_demo_data_df.withColumn('education_level', process_education_level_UDF(col('education_level')))\nprocessed_demo_data_df = processed_demo_data_df.withColumn('employment_status', process_employment_status_UDF(col('employment_status')))\nprocessed_demo_data_df = processed_demo_data_df.withColumn('living_status', process_living_status_UDF(col('living_status')))\nprocessed_demo_data_df = processed_demo_data_df.withColumn('ethnicity', process_ethnicity_UDF(col('ethnicity')))\nprocessed_demo_data_df = processed_demo_data_df.withColumn('presence_of_children', process_presence_of_children_UDF(col('number_of_children')))\n\ndashboard_demo_data_delta_file_path = \"/mnt/delta/general_data/Disqo_Dataset_Processed_Delta/dashboard_demo_data\"\nprocessed_demo_data_df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(dashboard_demo_data_delta_file_path)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5e178e70-e11d-44d5-9a09-0dd20d32ed29","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["%sql\ndrop table if exists disqo_dashboard_db.dashboard_demo_data;\ncreate TABLE disqo_dashboard_db.dashboard_demo_data \nUSING delta\nLOCATION \"/mnt/delta/general_data/Disqo_Dataset_Processed_Delta/dashboard_demo_data\""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"implicitDf":true},"nuid":"42350878-77da-4b7a-8ea4-7d9d5606d0e1","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["# Amazon Events Data Processing"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7b37c2e7-8039-4ad3-a266-adecf597c651","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["def get_asin_hierarchy_mapping(asin_mapping):\n#   asin_dict = defaultdict(str)\n#   asin_category = \"\"\n#   for row in asin_mapping:\n  hierarchy = \"\"\n#   if asin_mapping is not None:\n  hierarchy = asin_mapping.lstrip('\"[\"\"').rstrip('\"\"]\"').replace(\";\", \",\")\n  hierarchy = re.sub('\\\\\\\\n#[0-9]+ in\\\\xa0[a-zA-Z-&,\\s]+', ' ', hierarchy)\n  hierarchy = re.sub('\\((.*)\\)', \"\", hierarchy)\n  hierarchy = hierarchy.strip().replace('\",\"', ' -> ').replace(\" > \", ' -> ')\n  #  for simplicity, only add the longest one as the candidate\n#     if len(hierarchy) > len(asin_category):\n#       asin_category = hierarchy\n  return hierarchy\n\ndef get_node_reference(bn_df_list):\n  reference = {}\n  for row in bn_df_list:\n    keys = row[\"Name\"].lower()\n    value = row[\"Breadcrumb\"].lower()\n    value_regex = value.replace(' > ', ' ').replace(', ', ' ').replace(\" - \", ' ')\n    revised_keys = keys.replace(\", \", \" & \").split(\" & \")\n    revised_keys.append(keys)\n    for key in revised_keys:\n      if key not in reference:\n        reference[key] = []\n      reference[key].append((value, value_regex))\n  return reference\n\ndef jaccard(categoery_line, refer_s):\n  s1 = set(categoery_line.split(' '))\n  s2 = set(refer_s.split(' '))\n  score = round(len(s1.intersection(s2))*1.0 / len(s1.union(s2)), 2)\n  return score\n\ndef smith_waterman_similarity(categoery_line, refer_s, n):\n  score = round(smith_waterman.similarity(categoery_line, refer_s)*1.0 / n, 2)\n  return score\n\n\ndef category_linking(asin, category, reference):\n  max_score = 0\n  selected_path = ''\n  # \n  # if this 'category' is right in the reference key\n  if category in reference:\n      for path, revised_path in reference[category]:\n        score = jaccard(category, revised_path)\n        if score > max_score:\n          max_score = score\n          selected_path = path\n  # this 'category' is not in the reference key\n  else:\n    len_category = len(category)\n    for subcategory in category.split(\" -> \")[::-1]:\n      # check if the whole subcategory is in the reference, break if we find any one available\n      if subcategory in reference:\n        for path, revised_path in reference[subcategory]:\n          score = jaccard(category, revised_path)\n          if score > max_score:\n            max_score = score\n            selected_path = path\n        break\n      else:\n        split_subcategory = subcategory.split(\" \")[::-1]\n      \n        combined_category = ''\n        # from right to left to form new candiate category key to see if it is in the reference\n        for word in split_subcategory:\n          combined_category = word if combined_category == '' else word + ' ' + combined_category\n          # check if the combined subcategory keword is in the reference, break if we find any one available\n          if (not combined_category.startswith(\"&\")) and combined_category in reference:\n            for path, revised_path in reference[combined_category]:\n              score = smith_waterman_similarity(category, revised_path, len_category)\n              if score > max_score:\n                max_score = score\n                selected_path = path\n            break\n      \n  # create 8 columns as 8 levels for each asin \n  levels = [None, None, None, None, None, None, None, None]\n  if selected_path is not None:\n    for i, level in enumerate(selected_path.split(' > ')[:8]):\n      levels[i] = level\n        \n  return asin, levels[0], levels[1], levels[2]\n\ndef process_brand_name(name):\n  if name is None: return None\n  regexp1 = re.compile(r\"brand: ([\\w\\s]+)\")\n  regexp2 = re.compile(r\"visit the ([\\w\\s]+) store\")\n  if regexp1.search(name) is not None:\n    return regexp1.search(name).group(1)\n  elif regexp2.search(name) is not None:\n    return regexp2.search(name).group(1)\n  else:\n    return name\n\nprocess_brand_name_udf = udf(lambda z: process_brand_name(z), StringType())  "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7178b76e-47f0-4333-a4d2-7aec934b39b3","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Import Amazon Hierarchy Category Data as Reference"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2fc5d63f-98d8-46d2-93f7-ee7dccdd3c76","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# Aamazon Hierarchy Data\nfile_location = \"/FileStore/tables/amazon/amazon_browsenode.csv\"\nfile_type = \"csv\"\n\n# CSV options\ninfer_schema = \"false\"\nfirst_row_is_header = \"True\"\ndelimiter = \",\"\n\n# The applied options are for CSV files. For other file types, these will be ignored.\nbn_df = spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n  .load(file_location)\n\n# keywords -> path mapping reference processed from bn_df\nnode_reference = get_node_reference(bn_df.collect())"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2a2869a6-4c63-455c-af69-faec5b4f49dc","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Import raw events data"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c3bf4a20-0168-4522-8c65-4227f4866397","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["yesterday_date = 'date=' + (datetime.now().astimezone(timezone('US/Pacific')) - timedelta(days=1)).strftime(\"%Y-%m-%d\")\n\nevent_data_path = \"/mnt/delta/raw_data/Disqo_Dataset_Synced/v2/daily-export-data/\"+yesterday_date+\"/*.snappy.parquet\"\nevent_data = spark.read.option(\"header\", \"true\").parquet(event_data_path)\n# Manipulate timestamp\nevent_data_latest = event_data.withColumn(\"timestamp\", to_timestamp(col('timestamp'), \"yyyy-MM-dd HH\"))\nevent_data_latest = event_data_latest.withColumn(\"date\", to_date(col('timestamp')))\nevent_data_latest  = event_data_latest.withColumn(\"week_from_record\", floor_(datediff(col('date'),lit('2019-01-01'))/7)+1)\nevent_data_latest = event_data_latest.withColumn(\"product_brand\", process_brand_name_udf(col('product_brand')))\n# # ********************************remedy part if daily update is broken sometime*********************************************\n# event_data_path = \"/mnt/delta/raw_data/Disqo_Dataset_Synced/v2/daily-export-data/*/*.snappy.parquet\"\n# event_data = spark.read.option(\"header\", \"true\").parquet(event_data_path)\n\n# # Manipulate timestamp\n# event_data_latest = event_data.withColumn(\"timestamp\", to_timestamp(col('timestamp'), \"yyyy-MM-dd HH\"))\n# event_data_latest = event_data_latest.withColumn(\"date\", to_date(col('timestamp')))\n# event_data_latest = event_data_latest.withColumn(\"product_brand\", process_brand_name_udf(col('product_brand')))\n# event_data_latest  = event_data_latest.withColumn(\"week_from_record\", floor_(datediff(col('date'),lit('2019-01-01'))/7)+1)\n# #Only select data from the latest part for the further merging operation\n# event_data_latest = event_data_latest.filter((col(\"timestamp\") > lit(\"2023-01-04\")))\n# #***********remedy part if daily update is broken sometime*********************************************"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"07740931-4d21-4301-9297-e7a2be949a75","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["disqo_us_file_path = \"/mnt/delta/general_data/Disqo_Dataset_Processed_Delta/disqo_us\"\ndisqo_us_df = event_data_latest.filter(((col('timezone')=='America/New_York')|\n                                         (col('timezone')=='America/Chicago')|\n                                         (col('timezone')=='America/Los_Angeles')|\n                                         (col('timezone')=='America/Denver')|\n                                         (col('timezone')=='America/Indianapolis')|\n                                         (col('timezone')=='America/Anchorage')|\n                                         (col('timezone')=='America/Boise')|\n                                         (col('timezone')=='America/Indiana/Indianapolis')))\n\ndisqo_us_df.write.format(\"delta\").mode('append').save(disqo_us_file_path)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"97c3d61c-2e86-4b70-928c-60d6176e0d74","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["%sql\ndrop table if exists disqo_us;\nCREATE TABLE disqo_us \nUSING delta\nLOCATION \"/mnt/delta/general_data/Disqo_Dataset_Processed_Delta/disqo_us\""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"implicitDf":true},"nuid":"e98352b4-fbe0-45df-ba5e-96e6c4fe1f04","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# %sql\n# drop VIEW if exists disqo_dashboard_db.disqo_us_with_demo;\n# CREATE VIEW disqo_dashboard_db.disqo_us_with_demo as (\n#   select * from disqo_us left join disqo_demo_data using (user_id)\n# )"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"implicitDf":true},"nuid":"796ebc9e-f914-4b8e-98aa-e2672e30ccab","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# %sql\n# drop table if exists disqo_dashboard_db.disqo_us_mau_weekly;\n# create table disqo_dashboard_db.disqo_us_mau_weekly as(\n# with week_table as\n# (\n# select floor(datediff(date(timestamp),'2019-01-01')/7)+1 as week_from_record, min(date(timestamp)) as first_date_of_week\n# from disqo_us\n# group by floor(datediff(date(timestamp),'2019-01-01')/7)+1\n# ), daily_user_table as (\n# select distinct date(timestamp) as date, user_id from disqo_us\n# )\n# select w.week_from_record, w.first_date_of_week, count(distinct user_id) as prior_mau\n# from week_table w left join daily_user_table d \n# on datediff(w.first_date_of_week, d.date) > 1 and datediff(w.first_date_of_week, d.date) <= 30\n# group by w.week_from_record, w.first_date_of_week\n# order by w.week_from_record\n# );\n# update disqo_dashboard_db.disqo_us_mau_weekly\n# SET prior_mau = 40000\n# WHERE week_from_record = 1"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"implicitDf":true},"nuid":"877c440f-7ec6-4598-992e-bfa93b4f326c","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-3017602956510343&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      7</span>     display<span class=\"ansi-blue-fg\">(</span>df<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      8</span>     <span class=\"ansi-green-fg\">return</span> df\n<span class=\"ansi-green-fg\">----&gt; 9</span><span class=\"ansi-red-fg\">   </span>_sqldf <span class=\"ansi-blue-fg\">=</span> ____databricks_percent_sql<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     10</span> <span class=\"ansi-green-fg\">finally</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     11</span>   <span class=\"ansi-green-fg\">del</span> ____databricks_percent_sql\n\n<span class=\"ansi-green-fg\">&lt;command-3017602956510343&gt;</span> in <span class=\"ansi-cyan-fg\">____databricks_percent_sql</span><span class=\"ansi-blue-fg\">()</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      3</span>     <span class=\"ansi-green-fg\">import</span> base64\n<span class=\"ansi-green-intense-fg ansi-bold\">      4</span>     spark<span class=\"ansi-blue-fg\">.</span>sql<span class=\"ansi-blue-fg\">(</span>base64<span class=\"ansi-blue-fg\">.</span>standard_b64decode<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;ZHJvcCB0YWJsZSBpZiBleGlzdHMgZGlzcW9fZGFzaGJvYXJkX2RiLmRpc3FvX3VzX21hdV93ZWVrbHk=&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>decode<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">----&gt; 5</span><span class=\"ansi-red-fg\">     </span>spark<span class=\"ansi-blue-fg\">.</span>sql<span class=\"ansi-blue-fg\">(</span>base64<span class=\"ansi-blue-fg\">.</span>standard_b64decode<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;Y3JlYXRlIHRhYmxlIGRpc3FvX2Rhc2hib2FyZF9kYi5kaXNxb191c19tYXVfd2Vla2x5IGFzKAp3aXRoIHdlZWtfdGFibGUgYXMKKApzZWxlY3QgZmxvb3IoZGF0ZWRpZmYoZGF0ZSh0aW1lc3RhbXApLCcyMDE5LTAxLTAxJykvNykrMSBhcyB3ZWVrX2Zyb21fcmVjb3JkLCBtaW4oZGF0ZSh0aW1lc3RhbXApKSBhcyBmaXJzdF9kYXRlX29mX3dlZWsKZnJvbSBkaXNxb191cwpncm91cCBieSBmbG9vcihkYXRlZGlmZihkYXRlKHRpbWVzdGFtcCksJzIwMTktMDEtMDEnKS83KSsxCiksIGRhaWx5X3VzZXJfdGFibGUgYXMgKApzZWxlY3QgZGlzdGluY3QgZGF0ZSh0aW1lc3RhbXApIGFzIGRhdGUsIHVzZXJfaWQgZnJvbSBkaXNxb191cwopCnNlbGVjdCB3LndlZWtfZnJvbV9yZWNvcmQsIHcuZmlyc3RfZGF0ZV9vZl93ZWVrLCBjb3VudChkaXN0aW5jdCB1c2VyX2lkKSBhcyBwcmlvcl9tYXUKZnJvbSB3ZWVrX3RhYmxlIHcgbGVmdCBqb2luIGRhaWx5X3VzZXJfdGFibGUgZCAKb24gZGF0ZWRpZmYody5maXJzdF9kYXRlX29mX3dlZWssIGQuZGF0ZSkgPiAxIGFuZCBkYXRlZGlmZih3LmZpcnN0X2RhdGVfb2Zfd2VlaywgZC5kYXRlKSA8PSAzMApncm91cCBieSB3LndlZWtfZnJvbV9yZWNvcmQsIHcuZmlyc3RfZGF0ZV9vZl93ZWVrCm9yZGVyIGJ5IHcud2Vla19mcm9tX3JlY29yZAop&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>decode<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      6</span>     df <span class=\"ansi-blue-fg\">=</span> spark<span class=\"ansi-blue-fg\">.</span>sql<span class=\"ansi-blue-fg\">(</span>base64<span class=\"ansi-blue-fg\">.</span>standard_b64decode<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;dXBkYXRlIGRpc3FvX2Rhc2hib2FyZF9kYi5kaXNxb191c19tYXVfd2Vla2x5ClNFVCBwcmlvcl9tYXUgPSA0MDAwMApXSEVSRSB3ZWVrX2Zyb21fcmVjb3JkID0gMQ==&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>decode<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      7</span>     display<span class=\"ansi-blue-fg\">(</span>df<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/session.py</span> in <span class=\"ansi-cyan-fg\">sql</span><span class=\"ansi-blue-fg\">(self, sqlQuery)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    775</span>         <span class=\"ansi-blue-fg\">[</span>Row<span class=\"ansi-blue-fg\">(</span>f1<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">,</span> f2<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">&#39;row1&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> Row<span class=\"ansi-blue-fg\">(</span>f1<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-cyan-fg\">2</span><span class=\"ansi-blue-fg\">,</span> f2<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">&#39;row2&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> Row<span class=\"ansi-blue-fg\">(</span>f1<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-cyan-fg\">3</span><span class=\"ansi-blue-fg\">,</span> f2<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">&#39;row3&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">]</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    776</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-fg\">--&gt; 777</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">return</span> DataFrame<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jsparkSession<span class=\"ansi-blue-fg\">.</span>sql<span class=\"ansi-blue-fg\">(</span>sqlQuery<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> self<span class=\"ansi-blue-fg\">.</span>_wrapped<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    778</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    779</span>     <span class=\"ansi-green-fg\">def</span> table<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> tableName<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1302</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1303</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">-&gt; 1304</span><span class=\"ansi-red-fg\">         return_value = get_return_value(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1305</span>             answer, self.gateway_client, self.target_id, self.name)\n<span class=\"ansi-green-intense-fg ansi-bold\">   1306</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    115</span>     <span class=\"ansi-green-fg\">def</span> deco<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    116</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 117</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    118</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    119</span>             converted <span class=\"ansi-blue-fg\">=</span> convert_exception<span class=\"ansi-blue-fg\">(</span>e<span class=\"ansi-blue-fg\">.</span>java_exception<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    324</span>             value <span class=\"ansi-blue-fg\">=</span> OUTPUT_CONVERTER<span class=\"ansi-blue-fg\">[</span>type<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">(</span>answer<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">2</span><span class=\"ansi-blue-fg\">:</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> gateway_client<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    325</span>             <span class=\"ansi-green-fg\">if</span> answer<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-blue-fg\">==</span> REFERENCE_TYPE<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 326</span><span class=\"ansi-red-fg\">                 raise Py4JJavaError(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    328</span>                     format(target_id, &#34;.&#34;, name), value)\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling o326.sql.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:606)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:360)\n\tat com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.$anonfun$writeFiles$8(TransactionalWriteEdge.scala:427)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:239)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:386)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:186)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:968)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:141)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:336)\n\tat com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.$anonfun$writeFiles$1(TransactionalWriteEdge.scala:362)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:156)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:143)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.withOperationTypeTag(OptimisticTransaction.scala:104)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperation$6(DeltaLogging.scala:121)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:171)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:169)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordFrameProfile(OptimisticTransaction.scala:104)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperation$5(DeltaLogging.scala:120)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:330)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:424)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:444)\n\tat com.databricks.logging.Log4jUsageLoggingShim$.$anonfun$withAttributionContext$1(Log4jUsageLoggingShim.scala:33)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:94)\n\tat com.databricks.logging.Log4jUsageLoggingShim$.withAttributionContext(Log4jUsageLoggingShim.scala:31)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:205)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:204)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:20)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:240)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:225)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:20)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:419)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:339)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:20)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:330)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:302)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:20)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:57)\n\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:137)\n\tat com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:73)\n\tat com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:60)\n\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:98)\n\tat com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:431)\n\tat com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:410)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordOperation(OptimisticTransaction.scala:104)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:119)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:104)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordDeltaOperation(OptimisticTransaction.scala:104)\n\tat com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.$anonfun$recordWriteFilesOperation$1(TransactionalWriteEdge.scala:210)\n\tat com.databricks.sql.acl.CheckPermissions$.trusted(CheckPermissions.scala:1585)\n\tat com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.recordWriteFilesOperation(TransactionalWriteEdge.scala:209)\n\tat com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFiles(TransactionalWriteEdge.scala:231)\n\tat com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFiles$(TransactionalWriteEdge.scala:225)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:104)\n\tat com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFiles(TransactionalWriteEdge.scala:475)\n\tat com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFiles$(TransactionalWriteEdge.scala:466)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:104)\n\tat com.databricks.sql.transaction.tahoe.files.TransactionalWrite.writeFiles(TransactionalWrite.scala:215)\n\tat com.databricks.sql.transaction.tahoe.files.TransactionalWrite.writeFiles$(TransactionalWrite.scala:212)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:104)\n\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDelta.write(WriteIntoDelta.scala:327)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.$anonfun$run$2(CreateDeltaTableCommand.scala:142)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:156)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:143)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.withOperationTypeTag(CreateDeltaTableCommand.scala:54)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperation$6(DeltaLogging.scala:121)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:171)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:169)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordFrameProfile(CreateDeltaTableCommand.scala:54)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperation$5(DeltaLogging.scala:120)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:330)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:424)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:444)\n\tat com.databricks.logging.Log4jUsageLoggingShim$.$anonfun$withAttributionContext$1(Log4jUsageLoggingShim.scala:33)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:94)\n\tat com.databricks.logging.Log4jUsageLoggingShim$.withAttributionContext(Log4jUsageLoggingShim.scala:31)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:205)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:204)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:20)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:240)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:225)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:20)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:419)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:339)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:20)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:330)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:302)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:20)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:57)\n\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:137)\n\tat com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:73)\n\tat com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:60)\n\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:98)\n\tat com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:431)\n\tat com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:410)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordOperation(CreateDeltaTableCommand.scala:54)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:119)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:104)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordDeltaOperation(CreateDeltaTableCommand.scala:54)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.run(CreateDeltaTableCommand.scala:118)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.com$databricks$sql$transaction$tahoe$catalog$DeltaCatalog$$createDeltaTable(DeltaCatalog.scala:215)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog$StagedDeltaTableV2.commitStagedChanges(DeltaCatalog.scala:606)\n\tat org.apache.spark.sql.execution.datasources.v2.TableWriteExecHelper.$anonfun$writeToTable$1(WriteToDataSourceV2Exec.scala:504)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1715)\n\tat org.apache.spark.sql.execution.datasources.v2.TableWriteExecHelper.writeToTable(WriteToDataSourceV2Exec.scala:485)\n\tat org.apache.spark.sql.execution.datasources.v2.TableWriteExecHelper.writeToTable$(WriteToDataSourceV2Exec.scala:480)\n\tat org.apache.spark.sql.execution.datasources.v2.AtomicCreateTableAsSelectExec.writeToTable(WriteToDataSourceV2Exec.scala:110)\n\tat org.apache.spark.sql.execution.datasources.v2.AtomicCreateTableAsSelectExec.run(WriteToDataSourceV2Exec.scala:134)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:160)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:239)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:386)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:186)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:968)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:141)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:336)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:160)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:156)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:575)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:167)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:575)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:268)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:264)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:551)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:156)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:324)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:156)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:141)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:132)\n\tat org.apache.spark.sql.Dataset.&lt;init&gt;(Dataset.scala:225)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:104)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:968)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:101)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:803)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:968)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:798)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.sql.execution.OutOfMemorySparkException: Size of broadcasted table far exceeds estimates and exceeds limit of spark.driver.maxResultSize=4294967296. You can disable broadcasts for this query using set spark.sql.autoBroadcastJoinThreshold=-1\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.$anonfun$relationFuture$1(BroadcastExchangeExec.scala:202)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:968)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$4(SQLExecution.scala:448)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$3(SQLExecution.scala:448)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$2(SQLExecution.scala:447)\n\tat org.apache.spark.sql.execution.SQLExecution$.withOptimisticTransaction(SQLExecution.scala:465)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$1(SQLExecution.scala:446)\n\tat java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:104)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:68)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:54)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:101)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:104)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n</div>","errorSummary":"org.apache.spark.SparkException: Job aborted.","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-3017602956510343&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      7</span>     display<span class=\"ansi-blue-fg\">(</span>df<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      8</span>     <span class=\"ansi-green-fg\">return</span> df\n<span class=\"ansi-green-fg\">----&gt; 9</span><span class=\"ansi-red-fg\">   </span>_sqldf <span class=\"ansi-blue-fg\">=</span> ____databricks_percent_sql<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     10</span> <span class=\"ansi-green-fg\">finally</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     11</span>   <span class=\"ansi-green-fg\">del</span> ____databricks_percent_sql\n\n<span class=\"ansi-green-fg\">&lt;command-3017602956510343&gt;</span> in <span class=\"ansi-cyan-fg\">____databricks_percent_sql</span><span class=\"ansi-blue-fg\">()</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      3</span>     <span class=\"ansi-green-fg\">import</span> base64\n<span class=\"ansi-green-intense-fg ansi-bold\">      4</span>     spark<span class=\"ansi-blue-fg\">.</span>sql<span class=\"ansi-blue-fg\">(</span>base64<span class=\"ansi-blue-fg\">.</span>standard_b64decode<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;ZHJvcCB0YWJsZSBpZiBleGlzdHMgZGlzcW9fZGFzaGJvYXJkX2RiLmRpc3FvX3VzX21hdV93ZWVrbHk=&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>decode<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">----&gt; 5</span><span class=\"ansi-red-fg\">     </span>spark<span class=\"ansi-blue-fg\">.</span>sql<span class=\"ansi-blue-fg\">(</span>base64<span class=\"ansi-blue-fg\">.</span>standard_b64decode<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;Y3JlYXRlIHRhYmxlIGRpc3FvX2Rhc2hib2FyZF9kYi5kaXNxb191c19tYXVfd2Vla2x5IGFzKAp3aXRoIHdlZWtfdGFibGUgYXMKKApzZWxlY3QgZmxvb3IoZGF0ZWRpZmYoZGF0ZSh0aW1lc3RhbXApLCcyMDE5LTAxLTAxJykvNykrMSBhcyB3ZWVrX2Zyb21fcmVjb3JkLCBtaW4oZGF0ZSh0aW1lc3RhbXApKSBhcyBmaXJzdF9kYXRlX29mX3dlZWsKZnJvbSBkaXNxb191cwpncm91cCBieSBmbG9vcihkYXRlZGlmZihkYXRlKHRpbWVzdGFtcCksJzIwMTktMDEtMDEnKS83KSsxCiksIGRhaWx5X3VzZXJfdGFibGUgYXMgKApzZWxlY3QgZGlzdGluY3QgZGF0ZSh0aW1lc3RhbXApIGFzIGRhdGUsIHVzZXJfaWQgZnJvbSBkaXNxb191cwopCnNlbGVjdCB3LndlZWtfZnJvbV9yZWNvcmQsIHcuZmlyc3RfZGF0ZV9vZl93ZWVrLCBjb3VudChkaXN0aW5jdCB1c2VyX2lkKSBhcyBwcmlvcl9tYXUKZnJvbSB3ZWVrX3RhYmxlIHcgbGVmdCBqb2luIGRhaWx5X3VzZXJfdGFibGUgZCAKb24gZGF0ZWRpZmYody5maXJzdF9kYXRlX29mX3dlZWssIGQuZGF0ZSkgPiAxIGFuZCBkYXRlZGlmZih3LmZpcnN0X2RhdGVfb2Zfd2VlaywgZC5kYXRlKSA8PSAzMApncm91cCBieSB3LndlZWtfZnJvbV9yZWNvcmQsIHcuZmlyc3RfZGF0ZV9vZl93ZWVrCm9yZGVyIGJ5IHcud2Vla19mcm9tX3JlY29yZAop&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>decode<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      6</span>     df <span class=\"ansi-blue-fg\">=</span> spark<span class=\"ansi-blue-fg\">.</span>sql<span class=\"ansi-blue-fg\">(</span>base64<span class=\"ansi-blue-fg\">.</span>standard_b64decode<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;dXBkYXRlIGRpc3FvX2Rhc2hib2FyZF9kYi5kaXNxb191c19tYXVfd2Vla2x5ClNFVCBwcmlvcl9tYXUgPSA0MDAwMApXSEVSRSB3ZWVrX2Zyb21fcmVjb3JkID0gMQ==&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>decode<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      7</span>     display<span class=\"ansi-blue-fg\">(</span>df<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/session.py</span> in <span class=\"ansi-cyan-fg\">sql</span><span class=\"ansi-blue-fg\">(self, sqlQuery)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    775</span>         <span class=\"ansi-blue-fg\">[</span>Row<span class=\"ansi-blue-fg\">(</span>f1<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">,</span> f2<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">&#39;row1&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> Row<span class=\"ansi-blue-fg\">(</span>f1<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-cyan-fg\">2</span><span class=\"ansi-blue-fg\">,</span> f2<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">&#39;row2&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> Row<span class=\"ansi-blue-fg\">(</span>f1<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-cyan-fg\">3</span><span class=\"ansi-blue-fg\">,</span> f2<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">&#39;row3&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">]</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    776</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-fg\">--&gt; 777</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">return</span> DataFrame<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jsparkSession<span class=\"ansi-blue-fg\">.</span>sql<span class=\"ansi-blue-fg\">(</span>sqlQuery<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> self<span class=\"ansi-blue-fg\">.</span>_wrapped<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    778</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    779</span>     <span class=\"ansi-green-fg\">def</span> table<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> tableName<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1302</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1303</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">-&gt; 1304</span><span class=\"ansi-red-fg\">         return_value = get_return_value(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1305</span>             answer, self.gateway_client, self.target_id, self.name)\n<span class=\"ansi-green-intense-fg ansi-bold\">   1306</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    115</span>     <span class=\"ansi-green-fg\">def</span> deco<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    116</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 117</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    118</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    119</span>             converted <span class=\"ansi-blue-fg\">=</span> convert_exception<span class=\"ansi-blue-fg\">(</span>e<span class=\"ansi-blue-fg\">.</span>java_exception<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    324</span>             value <span class=\"ansi-blue-fg\">=</span> OUTPUT_CONVERTER<span class=\"ansi-blue-fg\">[</span>type<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">(</span>answer<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">2</span><span class=\"ansi-blue-fg\">:</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> gateway_client<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    325</span>             <span class=\"ansi-green-fg\">if</span> answer<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-blue-fg\">==</span> REFERENCE_TYPE<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 326</span><span class=\"ansi-red-fg\">                 raise Py4JJavaError(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    328</span>                     format(target_id, &#34;.&#34;, name), value)\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling o326.sql.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:606)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:360)\n\tat com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.$anonfun$writeFiles$8(TransactionalWriteEdge.scala:427)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:239)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:386)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:186)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:968)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:141)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:336)\n\tat com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.$anonfun$writeFiles$1(TransactionalWriteEdge.scala:362)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:156)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:143)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.withOperationTypeTag(OptimisticTransaction.scala:104)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperation$6(DeltaLogging.scala:121)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:171)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:169)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordFrameProfile(OptimisticTransaction.scala:104)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperation$5(DeltaLogging.scala:120)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:330)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:424)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:444)\n\tat com.databricks.logging.Log4jUsageLoggingShim$.$anonfun$withAttributionContext$1(Log4jUsageLoggingShim.scala:33)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:94)\n\tat com.databricks.logging.Log4jUsageLoggingShim$.withAttributionContext(Log4jUsageLoggingShim.scala:31)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:205)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:204)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:20)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:240)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:225)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:20)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:419)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:339)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:20)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:330)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:302)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:20)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:57)\n\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:137)\n\tat com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:73)\n\tat com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:60)\n\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:98)\n\tat com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:431)\n\tat com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:410)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordOperation(OptimisticTransaction.scala:104)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:119)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:104)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordDeltaOperation(OptimisticTransaction.scala:104)\n\tat com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.$anonfun$recordWriteFilesOperation$1(TransactionalWriteEdge.scala:210)\n\tat com.databricks.sql.acl.CheckPermissions$.trusted(CheckPermissions.scala:1585)\n\tat com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.recordWriteFilesOperation(TransactionalWriteEdge.scala:209)\n\tat com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFiles(TransactionalWriteEdge.scala:231)\n\tat com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFiles$(TransactionalWriteEdge.scala:225)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:104)\n\tat com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFiles(TransactionalWriteEdge.scala:475)\n\tat com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFiles$(TransactionalWriteEdge.scala:466)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:104)\n\tat com.databricks.sql.transaction.tahoe.files.TransactionalWrite.writeFiles(TransactionalWrite.scala:215)\n\tat com.databricks.sql.transaction.tahoe.files.TransactionalWrite.writeFiles$(TransactionalWrite.scala:212)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:104)\n\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDelta.write(WriteIntoDelta.scala:327)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.$anonfun$run$2(CreateDeltaTableCommand.scala:142)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:156)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:143)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.withOperationTypeTag(CreateDeltaTableCommand.scala:54)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperation$6(DeltaLogging.scala:121)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:171)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:169)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordFrameProfile(CreateDeltaTableCommand.scala:54)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperation$5(DeltaLogging.scala:120)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:330)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:424)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:444)\n\tat com.databricks.logging.Log4jUsageLoggingShim$.$anonfun$withAttributionContext$1(Log4jUsageLoggingShim.scala:33)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:94)\n\tat com.databricks.logging.Log4jUsageLoggingShim$.withAttributionContext(Log4jUsageLoggingShim.scala:31)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:205)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:204)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:20)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:240)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:225)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:20)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:419)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:339)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:20)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:330)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:302)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:20)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:57)\n\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:137)\n\tat com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:73)\n\tat com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:60)\n\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:98)\n\tat com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:431)\n\tat com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:410)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordOperation(CreateDeltaTableCommand.scala:54)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:119)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:104)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordDeltaOperation(CreateDeltaTableCommand.scala:54)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.run(CreateDeltaTableCommand.scala:118)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.com$databricks$sql$transaction$tahoe$catalog$DeltaCatalog$$createDeltaTable(DeltaCatalog.scala:215)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog$StagedDeltaTableV2.commitStagedChanges(DeltaCatalog.scala:606)\n\tat org.apache.spark.sql.execution.datasources.v2.TableWriteExecHelper.$anonfun$writeToTable$1(WriteToDataSourceV2Exec.scala:504)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1715)\n\tat org.apache.spark.sql.execution.datasources.v2.TableWriteExecHelper.writeToTable(WriteToDataSourceV2Exec.scala:485)\n\tat org.apache.spark.sql.execution.datasources.v2.TableWriteExecHelper.writeToTable$(WriteToDataSourceV2Exec.scala:480)\n\tat org.apache.spark.sql.execution.datasources.v2.AtomicCreateTableAsSelectExec.writeToTable(WriteToDataSourceV2Exec.scala:110)\n\tat org.apache.spark.sql.execution.datasources.v2.AtomicCreateTableAsSelectExec.run(WriteToDataSourceV2Exec.scala:134)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:160)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:239)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:386)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:186)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:968)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:141)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:336)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:160)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:156)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:575)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:167)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:575)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:268)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:264)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:551)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:156)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:324)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:156)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:141)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:132)\n\tat org.apache.spark.sql.Dataset.&lt;init&gt;(Dataset.scala:225)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:104)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:968)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:101)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:803)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:968)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:798)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.sql.execution.OutOfMemorySparkException: Size of broadcasted table far exceeds estimates and exceeds limit of spark.driver.maxResultSize=4294967296. You can disable broadcasts for this query using set spark.sql.autoBroadcastJoinThreshold=-1\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.$anonfun$relationFuture$1(BroadcastExchangeExec.scala:202)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:968)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$4(SQLExecution.scala:448)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$3(SQLExecution.scala:448)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$2(SQLExecution.scala:447)\n\tat org.apache.spark.sql.execution.SQLExecution$.withOptimisticTransaction(SQLExecution.scala:465)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$1(SQLExecution.scala:446)\n\tat java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:104)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:68)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:54)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:101)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:104)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# %sql\n# drop table if exists disqo_dashboard_db.disqo_us_mau_monthly;\n# create table disqo_dashboard_db.disqo_us_mau_monthly as(\n# select year(timestamp) as year, month(timestamp) as month, min(date(timestamp)) as first_date_of_month, count(distinct user_id) as mau\n# from disqo_us\n# group by year(timestamp), month(timestamp)\n# )"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"implicitDf":true},"nuid":"b6ae6c1b-e4b9-403e-b676-7c9fbf0b813a","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"num_affected_rows","type":"\"long\"","metadata":"{}"},{"name":"num_inserted_rows","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>num_affected_rows</th><th>num_inserted_rows</th></tr></thead><tbody></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Load reference data"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"67deebab-5931-42b0-a4c8-7cd7bef512dd","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# load the asin hierarchy mapping data\nmain_asin_hierarchy_mapping_path =  \"/mnt/delta/general_data/Disqo_Dataset_Processed_Delta/asin_hierarchy_mapping\"\nmain_asin_hierarchy_mapping_df = spark.read.option(\"header\", \"true\").format(\"delta\").load(main_asin_hierarchy_mapping_path)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2757b481-1621-484a-949d-6fc699f1d13c","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["expr1 = r'^[0-9A-Z]+$'\nasin_hierarchy_list = event_data_latest.filter((event_data_latest.page_domain == 'amazon.com') & (event_data_latest.asin.rlike(expr1)) & (event_data_latest.category_hierarchy.isNotNull()))\\\n                                    .select(col(\"asin\"), col(\"category_hierarchy\"))\\\n                                       .drop_duplicates()     "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"59eb8bd2-a58e-41b4-b125-5da3a94379ea","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Select only new data to be processed"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"87cfbf76-fe7c-40d1-80d1-b8fdf040fe26","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# # Only get new asin to be processed\nmain_asin_keys = main_asin_hierarchy_mapping_df.select(\"asin\")\nasin_keys = asin_hierarchy_list.select(\"asin\")\nnew_asin = asin_keys.subtract(main_asin_keys)\nprint(\"There are {} asin records in the database\".format(main_asin_keys.distinct().count()))\nprint(\"There are {} asin records for this new batch\".format(asin_keys.distinct().count()))\nn_new_asin = new_asin.count()\nprint(\"There are {} new asin that are to be processed\".format(n_new_asin))\nasin_rdd  = asin_hierarchy_list.join(new_asin, 'asin', 'inner').rdd"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"593dcf05-4afe-4d52-812c-c0351afea179","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">There are 13925816 asin records in the database\nThere are 23764 asin records for this new batch\nThere are 5678 new asin that are to be processed\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">There are 13925816 asin records in the database\nThere are 23764 asin records for this new batch\nThere are 5678 new asin that are to be processed\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Processing of category linking"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5248c444-b951-41bd-a25f-2846aaf5d9e6","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# Do the matching process for the incremental part of the asins\nasin_hierarchy = asin_rdd.map(lambda x: (x[\"asin\"], get_asin_hierarchy_mapping(x[\"category_hierarchy\"])))\\\n                            .reduceByKey(lambda x, y: x if len(x)>len(y) else y)\\\n                                  .map(lambda row: category_linking(row[0], row[1], node_reference))\nasinSchema = StructType([       \n    StructField('asin', StringType(), True),\n    StructField('level_0', StringType(), True),\n    StructField('level_1', StringType(), True),\n    StructField('level_2', StringType(), True)\n])\nasin_hierarchy_mapping_df = spark.createDataFrame(asin_hierarchy, schema = asinSchema)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0617e2b3-26cf-4a09-8a28-0a7efe9ce3bb","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Update asin - hierarchy mapping data on exisiting dataset"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a0c53033-0ffc-4aeb-9b48-8dcf80b11cb6","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# event_data_delta_file_path = \"/mnt/delta/general_data/Disqo_Dataset_Processed_Delta/amazon_with_category\"\nmain_asin_hierarchy_mapping_df = main_asin_hierarchy_mapping_df.union(asin_hierarchy_mapping_df)\nasin_hierarchy_mapping_df.write.format(\"delta\").mode(\"append\").save(main_asin_hierarchy_mapping_path)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a5237e85-9e72-4fa2-b72c-993cbf5b340d","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Append new batch of event data into the original event dataset"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"fb942bbf-0df4-4a5f-9387-6ffeb741d338","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# temperarily processing whole bactch data\n# Join this new event data with the asin hierarchy mapping to get complete new chunk of dataset\namazon_df = event_data_latest.filter(((event_data.timezone=='America/New_York')|\n                                 (event_data.timezone=='America/Chicago')|\n                                 (event_data.timezone=='America/Los_Angeles')|\n                                 (event_data.timezone=='America/Denver')|\n                                 (event_data.timezone=='America/Indianapolis')|\n                                 (event_data.timezone=='America/Anchorage')|\n                                 (event_data.timezone=='America/Boise')|\n                                 (event_data.timezone=='America/Indiana/Indianapolis')) & (event_data.page_domain=='amazon.com'))\\\n                          .drop('category_categoryRanking')\\\n                                .dropDuplicates()\namazon_with_category_levels = amazon_df.join(main_asin_hierarchy_mapping_df, ['asin'], \"left\")                                                                 "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0ee890c9-93f0-4299-9518-54c50fa60a42","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["event_data_delta_file_path = \"/mnt/delta/general_data/Disqo_Dataset_Processed_Delta/amazon_with_category\"\namazon_with_category_levels.write.format(\"delta\").mode(\"append\").save(event_data_delta_file_path)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b85f6d80-4622-4d75-8a5b-e17db7c0379f","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["%sql\ndrop table if exists amazon_with_hierarchy;\n-- Headsup: Delete the exsisting table before creating this one\nCREATE TABLE amazon_with_hierarchy\nUSING delta\nLOCATION \"/mnt/delta/general_data/Disqo_Dataset_Processed_Delta/amazon_with_category\""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"implicitDf":true},"nuid":"ab31533c-d70c-426b-81a7-076c85ab1244","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["%sql\nDROP VIEW if exists disqo_dashboard_db.amazon_with_hierarchy_with_demo;\nCREATE VIEW disqo_dashboard_db.amazon_with_hierarchy_with_demo as (\n  select * from amazon_with_hierarchy left join disqo_demo_data using (user_id)\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"implicitDf":true},"nuid":"c4b0c191-2712-4007-965c-1427fe15b9f5","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Sync_Disqo_Data_increment_update","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2,"mostRecentlyExecutedCommandWithImplicitDF":{"commandId":3017602956510359,"dataframes":["_sqldf"]}},"language":"python","widgets":{},"notebookOrigID":3017602956510323}},"nbformat":4,"nbformat_minor":0}
